import os
import pathlib

import geopandas as gpd
import pandas as pd
import numpy as np

from metrics import compute_polis_lines, compute_dataset_metrics, compute_testset_f1_ap, compute_coverage
from vectorize import create_predictions_df, remove_outliers

cwd = pathlib.Path(os.getcwd())

configfile: cwd.parent / 'config.yaml'

config['Directories']['work_dir'] = pathlib.Path(config['Directories']['work_dir'])
for key, value in config['Directories'].items():
    if key != 'work_dir' or value != None:
        config['Directories'][key] = config['Directories']['work_dir'] / config['Directories'][key]

evaluation_df = gpd.read_file(config['Directories']['geom_dir'])

pathlib.Path(config['Directories']['output_vectors'] / config['Model_Details']['name']).mkdir(parents = True, exist_ok = True)

model_checkpoints = [checkpoint.name for checkpoint in (config['Directories']['nn_outputs'] / config['Model_Details']['name']).glob('epoch*')] # several checkpoints are saved for each model, corresponding to the different metrics being tracked
predictions_vector_dir = (config['Directories']['output_vectors'] / (config['Model_Details']['name'] + '/')).as_posix()

with open(config['Directories']['split']) as split_info:
    split = split_info.readlines()
    split = [elem.strip() for elem in split]

train_tiles = split[1:split.index('train_empties')] 
validation_tiles = split[split.index('validation_valids') + 1:split.index('validation_empties')]
test_tiles = split[split.index('test_valids') + 1:split.index('test_empties')]

if config['Data']['test_interferometric_only']:
    test_interferometric = split[split.index('interferometric_valids') + 1:split.index('interferometric_empties')]
    test_tiles = test_tiles + test_interferometric

predictions_npz = train_tiles + validation_tiles + test_tiles

rule all:
    input:
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}' + '.geojson', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_polis_distances.npy', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_metrics.npy', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered' + '.geojson', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered_polis_distances.npy', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered_metrics.npy', checkpoint = model_checkpoints),
        expand(predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_ml_metrics.npy', checkpoint = model_checkpoints)

rule vectorize_predictions_compute_metrics:
    input:
        config['Directories']['nn_outputs'] / (config['Model_Details']['name'] + '/{checkpoint}')
    output:
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}' + '.geojson',
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_polis_distances.npy',
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_metrics.npy',
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_ml_metrics.npy'
    run:
        opt_threshold = config['Data']['bin_threshold']
        tiles_dir = pathlib.Path(input[0])
        # Generate dictionary with prediction geometries
        predictions_df = create_predictions_df(gl_df = evaluation_df, tiles_dir = tiles_dir, labels_dir = config['Directories']['labels_tif'], opt_threshold = opt_threshold)

        # Compute PoLiS metrics of unfiltered predictions
        pred_to_gt, gt_to_pred, overall_deviations = compute_polis_lines(evaluation_df, predictions_df)
        line_metrics = pd.DataFrame(columns = ['pred_to_gt', 'gt_to_pred', 'deviations', 'dataset', 'UUID'])
        line_metrics['pred_to_gt'] = pred_to_gt
        line_metrics['gt_to_pred'] = gt_to_pred
        line_metrics['deviations'] = overall_deviations
        line_metrics['UUID'] = predictions_df.UUID.values
        line_metrics['dataset'] = predictions_df.dataset.values

        line_metrics = compute_coverage(ground_truth = evaluation_df, prediction = predictions_df, line_metrics = line_metrics)
        
        # Compute overall dataset metrics
        compute_dataset_metrics(df = line_metrics, savemetrics = pathlib.Path(output[2]))
        
        predictions_df.to_file(output[0], driver = 'GeoJSON')
        np.save(output[1], line_metrics.to_dict())

        thresholds = np.linspace(0.1, 1, 10)

        predictions = []
        labels = []

        for tile in test_tiles:
            with np.load(tiles_dir / tile) as data:
                predictions.append(data['arr_0'][:, :, 0])
            with np.load(config['Directories']['labels_npz'] / tile) as data:
                labels.append(data['arr_0'][:, :, 0])

        ap, f1 = compute_testset_f1_ap(predictions = predictions, labels = labels, thresholds = thresholds)

        np.save(pathlib.Path(output[-1]), {'ap': ap, 'ods_f1': f1})

rule filter_predictions:
    input:
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}' + '.geojson'
    output:
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered' + '.geojson',
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered_polis_distances.npy',
        predictions_vector_dir + '/' + config['Model_Details']['name'] + '_{checkpoint}_filtered_metrics.npy'
    run:
        # Filter predictions, using measures as reference
        predictions_df = gpd.read_file(input[0])
        filtered_df = remove_outliers(predictions_df = predictions_df, reference = config['Directories']['reference_manual_GL'], 
        distance_threshold = config['Data']['outlier_removal_threshold'])
        filtered_df.to_file(output[0])


        # Compute PoLiS metrics of filtered predictions
        pred_to_gt, gt_to_pred, overall_deviations = compute_polis_lines(evaluation_df, filtered_df)
        line_metrics_filtered = pd.DataFrame(columns = ['pred_to_gt', 'gt_to_pred', 'deviations', 'dataset', 'UUID'])
        line_metrics_filtered['pred_to_gt'] = pred_to_gt
        line_metrics_filtered['gt_to_pred'] = gt_to_pred
        line_metrics_filtered['deviations'] = overall_deviations
        line_metrics_filtered['UUID'] = filtered_df.UUID.values
        line_metrics_filtered['dataset'] = filtered_df.dataset.values

        line_metrics_filtered = compute_coverage(ground_truth = evaluation_df, prediction = filtered_df, line_metrics = line_metrics_filtered)
        compute_dataset_metrics(df = line_metrics_filtered, savemetrics = pathlib.Path(output[-1]))
        np.save(output[1], line_metrics_filtered.to_dict())